{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "import os\n",
    "%pylab notebook\n",
    "lib_path = '/home/fgeigl/navigability_of_networks'\n",
    "sys.path.append(lib_path)\n",
    "lib_path = '/home/fgeigl/navigability_of_networks/tools'\n",
    "sys.path.append(lib_path)\n",
    "import network_matrix_tools\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from collections import Counter\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numba\n",
    "from joblib import Parallel, delayed\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = load_sparse_csr('/opt/datasets/wiki_clickstream/adjacency_clickstream_network_largest_component.npz')\n",
    "B = load_sparse_csr('/opt/datasets/wiki_clickstream/clickstream_network_transition_bias_largest_component.npz')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: <class 'scipy.sparse.csr.csr_matrix'> (2140423, 2140423)\n",
      "B: <class 'scipy.sparse.csr.csr_matrix'> (2140423, 2140423)\n"
     ]
    }
   ],
   "source": [
    "print('A:', type(A), A.shape)\n",
    "print('B:', type(B), B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest component contains 2140423 nodes ( 1.0 )\n",
      "A: <class 'scipy.sparse.csr.csr_matrix'> (2140423, 2140423) 3.59513059841e-05\n",
      "B: <class 'scipy.sparse.csr.csr_matrix'> (2140423, 2140423) 2.66174133129e-06\n"
     ]
    }
   ],
   "source": [
    "_, labels = connected_components(A + B, directed=True, connection='strong', return_labels=True)\n",
    "label_counts = Counter(labels)\n",
    "largest_label, num_nodes = max(label_counts.items(), key=operator.itemgetter(1))\n",
    "print('largest component contains', num_nodes, 'nodes', '(', num_nodes/A.shape[0], ')')\n",
    "if num_nodes != A.shape[0]:\n",
    "    label_filt = labels == largest_label\n",
    "    A = A[label_filt, :][:, label_filt]\n",
    "    B = B[label_filt, :][:, label_filt]\n",
    "print('A:', type(A), A.shape, A.nnz/(np.power(A.shape[0],2)))\n",
    "print('B:', type(B), B.shape, B.nnz/(np.power(B.shape[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunks(my_range, num_chunks):\n",
    "    chunk_len = int(len(my_range) / num_chunks)\n",
    "    if len(my_range) % num_chunks > 0:\n",
    "        chunk_len += 1\n",
    "    return [my_range[i*chunk_len:(i+1)*chunk_len if i < num_chunks -1 else None] for i in range(num_chunks)]\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def part_dot(M, pi):\n",
    "    return M.dot(pi)\n",
    "\n",
    "@numba.jit\n",
    "def normalize(pi_vec):\n",
    "    norm = 1./(sqrt(np.power(pi_vec, 2).sum()))\n",
    "    pi_vec *= norm\n",
    "    return pi_vec, norm\n",
    "\n",
    "class ParallelDot:\n",
    "    def __init__(self, M_chunks):\n",
    "        self.M_chunks = M_chunks\n",
    "        self.worker = Parallel(n_jobs=len(M_chunks), batch_size=1, backend='threading')\n",
    "    \n",
    "    def dot(self, pi):\n",
    "        return np.hstack(self.worker(delayed(part_dot)(M_chunk, pi) for M_chunk in self.M_chunks))\n",
    "\n",
    "def stat_dist_power_iter(M, max_iter = 1e5, precision=10, init_vec = None, n_jobs=5):\n",
    "    dtype = np.float32 if precision < 7 else (np.float64 if precision < 16 else np.longdouble)\n",
    "    print('\\tusing dtye:', dtype)\n",
    "    print('\\tnormalize...', end='')\n",
    "    sys.stdout.flush()\n",
    "    max_iter = int(max_iter)\n",
    "    M = M.astype(dtype)\n",
    "    P = M.dot(diags(1. / np.array(M.sum(axis=0), dtype=dtype).flatten()))\n",
    "    if init_vec is None:\n",
    "        pi_vec = np.ones(P.shape[0], dtype=dtype) / P.shape[0]\n",
    "    else:\n",
    "        pi_vec = init_vec.astype(dtype)\n",
    "    print('done. ') #, type(pi_vec), pi_vec.dtype, type(P), P.dtype)\n",
    "    sys.stdout.flush()\n",
    "    diff = list()\n",
    "    precision = int(round(precision))\n",
    "    atol = np.power(1e1, -precision)\n",
    "    print('\\tgen chunk idx...', end='')\n",
    "    sys.stdout.flush()\n",
    "    chunk_idx = chunks(range(P.shape[0]), n_jobs)\n",
    "    assert chunk_idx[-1][-1] == P.shape[0] - 1\n",
    "    assert chunk_idx[0][0] == 0\n",
    "    assert sum(map(len, chunk_idx)) == P.shape[0]\n",
    "    print('done.') # chunk len:', map(len, chunk_idx))    \n",
    "    sys.stdout.flush()    \n",
    "    print('\\tslice matrix.', P.shape[0], end='...')\n",
    "    sys.stdout.flush()\n",
    "    P_row_chunks = [P[idx_range,:] for idx_range in chunk_idx]\n",
    "    par_dot = ParallelDot(P_row_chunks)\n",
    "    print('done')\n",
    "    print('\\tstart power iterations. max. iterations:', max_iter)\n",
    "    sys.stdout.flush()\n",
    "    comp_times = list()\n",
    "    start = datetime.datetime.now()\n",
    "    diverge = 0\n",
    "    for i in range(1, max_iter + 1):\n",
    "        now = datetime.datetime.now()\n",
    "        comp_times.append((now-start).total_seconds())\n",
    "        comp_times = comp_times[-25:]\n",
    "        avg_iter_time = sum(comp_times)/len(comp_times)\n",
    "        start = now\n",
    "        pi_vec, last_vec = par_dot.dot(pi_vec), pi_vec\n",
    "        # print(pi_vec)\n",
    "        pi_vec, norm = normalize(pi_vec)\n",
    "        current_diff = np.absolute(last_vec - pi_vec).max()\n",
    "        diff.append(current_diff)\n",
    "        if len(diff) > 10:\n",
    "            mean_diff_data = diff[-11:-1]\n",
    "            mean_diff = sum(mean_diff_data) / len(mean_diff_data)\n",
    "            improvement = mean_diff - current_diff\n",
    "            if improvement < 0:\n",
    "                diverge += 1\n",
    "                if diverge > 10:\n",
    "                    print('\\n!!!diverge...stopping...')\n",
    "                    break\n",
    "            elif mean_diff < atol or improvement * 100. < atol:\n",
    "                    break\n",
    "            else:\n",
    "                diverge = 0\n",
    "            #time_remain = (current_diff / improvement) * avg_iter_time\n",
    "            print('\\r\\titeration:', (\"%.0f\" % i).rjust(len(str(max_iter)), '0') ,(\"|| abs iter diff: %.\" + str(precision) + 'f') % mean_diff,\n",
    "                  (\"|| %.3f\" % avg_iter_time), 'sec/it', end='')\n",
    "                  # '|| min. remain:', datetime.timedelta(seconds=int(time_remain)), end='')\n",
    "        else:\n",
    "            print('\\r\\titeration:', i, end='')\n",
    "        sys.stdout.flush()\n",
    "    if i == max_iter:\n",
    "        print('\\n\\tdid not converge within', i, 'iterations!!!')\n",
    "    else:\n",
    "        print('\\n\\tneeded', i, 'iterations')\n",
    "    print('\\tlast diff:', (\" %.\" + str(precision) + 'f') % current_diff)\n",
    "    print('\\tlargest eigval:', (\" %.\" + str(precision) + 'f') % norm)\n",
    "    print('\\tsmallest value:', pi_vec.min())\n",
    "    assert len(pi_vec) == P.shape[0]\n",
    "    return pi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded stored data: Index([u'beta_0.001', u'beta_0.005', u'beta_0.01', u'beta_0.05', u'beta_0.1',\n",
      "       u'beta_0.25', u'beta_0.5', u'beta_0.75', u'beta_1.0', u'A_sd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_fname = 'click_stream_results_poweriter_10k_p10.df'\n",
    "time_df_fname = 'click_stream_times_poweriter_10k_p10.df'\n",
    "try:\n",
    "    df = pd.read_pickle(df_fname)\n",
    "    times = pd.read_pickle(time_df_fname)\n",
    "    print('loaded stored data:', df.columns)\n",
    "except:\n",
    "    print('init new data')\n",
    "    df = pd.DataFrame()\n",
    "    times = pd.Series()\n",
    "\n",
    "# stat_dist_power_iter(A)\n",
    "A = A.astype(np.longdouble)\n",
    "B = B.astype(np.longdouble)\n",
    "\n",
    "if 'A_sd' not in df.columns:\n",
    "    start = datetime.datetime.now()\n",
    "    #_, df['A_sd'] = network_matrix_tools.calc_entropy_and_stat_dist(A, method='EV', smooth_bias=False, calc_entropy_rate=False)\n",
    "    df['A_sd'] = stat_dist_power_iter(A)\n",
    "    times.loc['A_sd'] = datetime.datetime.now() - start\n",
    "    print(datetime.datetime.now() - start)\n",
    "init_vec = df['A_sd'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc beta: 1.0 already cached\n",
      "\ttook: 0 days 00:34:45.895689\n",
      "calc beta: 0.75 already cached\n",
      "\ttook: 0 days 00:39:09.559378\n",
      "calc beta: 0.5 already cached\n",
      "\ttook: 0 days 00:55:31.146890\n",
      "calc beta: 0.25 already cached\n",
      "\ttook: 0 days 01:33:32.803341\n",
      "calc beta: 0.1 already cached\n",
      "\ttook: 0 days 03:03:32.114711\n",
      "calc beta: 0.05 already cached\n",
      "\ttook: 0 days 05:21:16.522985\n",
      "calc beta: 0.01 already cached\n",
      "\ttook: 0 days 17:36:10.518916\n",
      "calc beta: 0.005 already cached\n",
      "\ttook: 0 days 18:55:16.007131\n",
      "calc beta: 0.001 already cached\n",
      "\ttook: 0 days 03:04:57.346407\n"
     ]
    }
   ],
   "source": [
    "for beta in [1., 0.75, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    col_name = 'beta_' + str(beta)\n",
    "    if col_name not in df.columns:\n",
    "        print('calc beta:', beta)\n",
    "        print('\\t', datetime.datetime.now())\n",
    "        start = datetime.datetime.now()\n",
    "        df[col_name] = stat_dist_power_iter((beta * A) + B.T, init_vec = init_vec)\n",
    "        times.loc[col_name] = datetime.datetime.now() - start\n",
    "        df.to_pickle(df_fname)\n",
    "        times.to_pickle(time_df_fname)\n",
    "    else:\n",
    "        print('calc beta:', beta, 'already cached')\n",
    "    print('\\ttook:', times.loc[col_name])\n",
    "    init_vec = df[col_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson:\n",
      "beta_0.001    0.031279\n",
      "beta_0.005    0.090264\n",
      "beta_0.01     0.176960\n",
      "beta_0.05     0.546975\n",
      "beta_0.1      0.649509\n",
      "beta_0.25     0.724052\n",
      "beta_0.5      0.772369\n",
      "beta_0.75     0.801053\n",
      "beta_1.0      0.821379\n",
      "A_sd          1.000000\n",
      "Name: A_sd, dtype: float64\n",
      "spearman:\n",
      "beta_0.001    0.536537\n",
      "beta_0.005    0.542527\n",
      "beta_0.01     0.548452\n",
      "beta_0.05     0.580206\n",
      "beta_0.1      0.606661\n",
      "beta_0.25     0.657440\n",
      "beta_0.5      0.706799\n",
      "beta_0.75     0.738492\n",
      "beta_1.0      0.761535\n",
      "A_sd          1.000000\n",
      "Name: A_sd, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('pearson:')\n",
    "print(df.corr(method='pearson').loc['A_sd'])\n",
    "print('spearman:')\n",
    "print(df.corr(method='spearman').loc['A_sd'])\n",
    "#print('kendall:')\n",
    "#print(df.corr(method='kendall').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_key(name):\n",
    "    val = name.rsplit('_', 1)[-1]\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return 100.\n",
    "\n",
    "sorted_cols = sorted(df.columns, key=sort_key)\n",
    "df = df[sorted_cols]\n",
    "df.to_pickle(df_fname)\n",
    "times.to_pickle(time_df_fname)\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
